# Logan-analysis

System to analyze Logan unitigs/contigs at scale with AWS Batch.

Adapted from https://github.com/ababaian/logan

## Warning / Costs

Running this system costs real \$'s in your AWS bill. Spot instances with local disk are 0.022$ per vCPU-hour (https://instances.vantage.sh/aws/ec2/c5d.4xlarge). E.g. a 10,000 vCPU workload during 10 hours is 2,200$ total. This corresponds roughly to a job capable of processing Logan compressed contigs at 1 MB per second per core. Do a pilot run and use AWS Cost Explorer 24 hours later to see real costs.


## Setup to run on cloud

You don't need to do this section if all you do is test the container locally. Read on if you're going to run analyses on cloud.

So far this setup has only been tested on `c5d` instances because tasks are relying on a local disk to download contig files.

- Ask Rayan to share `ami-09f62d2604cc5b8fe` with you, or make your own AMI with `awlcliv2`, or just include AWS CLI in the `Dockerfile` and use no AMI.

- Run `spinupd.sh` to deploy the Cloudformation stack and check your Cloudformation web Interface to make sure the stack is `CREATE_COMPLETE`.

- If needed to make adjustments to the stack, do them and run `spinupd.sh --update ` and check your Cloudformation to make sure the stack is `UPDATE_COMPLETE`.

## Checklist for running in production

Prepare your data in the `analyses/` folder, see previous runs for an example of file organization.

In the Ì€`batch/` folder:

0) Modify the beginning of `logan-analysis.sh` so that it does the task you want.

1) Modify the task itself in `task/` folder.

2) Modify `Dockerfile` to upload the desired references indexes.

3) Test the container with `test_docker.sh`. 

3) Run `deploy-docker.sh` to upload the container.


Go to the root folder of this repository. Pay attention to the output bucket names bucket hard coded in `run_*.sh` (`serratus-rayan`).


0) Modify the `vcpus` variable in `process_array.sh` to correct `jobdef`. 2 vcpus, i.e. `jobdef=logan-analysis-2c-job` should be fine, jobs will be 2 cores and 3.5 GB RAM, DIAMOND needs at least that.

1) Run `run_test.sh` to see that it works at all.

2) Run `run_pilot.sh` to get an estimate of the costs

2) Run `run_many.sh` for the big run on all Logan contigs.

Behind the scenes, these scripts call `process_array.sh [dest_bucket] [nb_jobs]`. Where `dest_bucket` is the name of the destination bucket, and `nb_jobs` is the number of jobs to submit (can't exceed 10000). The more jobs, the faster it will be. Destination bucket file structure is decided by the task.

## Running tests

Run `test_docker.sh` for a local test.

Modify and run `run_test.sh` for a Batch test job, then `run_pilot.sh` for an estimation of costs. Those scripts have a hardcoded output bucket name that needs to be changed.

## Cleanup

Manually delete the CloudFormation stack. Also delete the ECR image. 

